{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SRINIDHISAGI/NLP1/blob/main/NLP1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wdj9RMfoGPC2"
      },
      "cell_type": "markdown",
      "source": [
        "Colab is making it easier than ever to integrate powerful Generative AI capabilities into your projects. We are launching public preview for a simple and intuitive Python library (google.colab.ai) to access state-of-the-art language models directly within Pro and Pro+ subscriber Colab environments.  This means subscribers can spend less time on configuration and set up and more time bringing their ideas to life. With just a few lines of code, you can now perform a variety of tasks:\n",
        "- Generate text\n",
        "- Translate languages\n",
        "- Write creative content\n",
        "- Categorize text\n",
        "\n",
        "Happy Coding!\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/Getting_started_with_google_colab_ai.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "toy_corpus = [\n",
        "    \"low\", \"low\", \"low\", \"low\", \"low\",\n",
        "    \"lowest\", \"lowest\",\n",
        "    \"newer\", \"newer\", \"newer\", \"newer\", \"newer\", \"newer\",\n",
        "    \"wider\", \"wider\", \"wider\",\n",
        "    \"new\", \"new\"\n",
        "]\n",
        "corpus = [\" \".join(list(word)) + \" _\" for word in toy_corpus]\n",
        "corpus = [tuple(token.split(\" \")) for token in corpus]\n",
        "\n",
        "def get_stats(corpus):\n",
        "    pairs = defaultdict(int)\n",
        "    for word in corpus:\n",
        "        for i in range(len(word)-1):\n",
        "            pairs[(word[i], word[i+1])] += 1\n",
        "    return pairs\n",
        "\n",
        "def merge_pair(pair, corpus):\n",
        "    new_corpus = []\n",
        "    replacement = \"\".join(pair)\n",
        "    for word in corpus:\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            if i < len(word)-1 and (word[i], word[i+1]) == pair:\n",
        "                new_word.append(replacement)\n",
        "                i += 2\n",
        "            else:\n",
        "                 new_word.append(word[i])\n",
        "                 i += 1\n",
        "        new_corpus.append(tuple(new_word))\n",
        "    return new_corpus\n",
        "\n",
        "vocab = set([sym for word in corpus for sym in word])\n",
        "print(\"Initial vocab:\", vocab)\n",
        "\n",
        "for step in range(1, 4):\n",
        "    pairs = get_stats(corpus)\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    corpus = merge_pair(best, corpus)\n",
        "    vocab.add(\"\".join(best))\n",
        "    print(f\"\\nStep {step}: merge {best} → {''.join(best)}\")\n",
        "    print(\"Corpus sample:\", corpus[:8])\n",
        "    print(\"Updated vocab:\", vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz0cht0gk-wc",
        "outputId": "2029fbff-c957-4756-fc9d-baa929d303d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial vocab: {'l', 'w', 'i', 'o', 'n', 's', 'e', 't', 'r', '_', 'd'}\n",
            "\n",
            "Step 1: merge ('e', 'r') → er\n",
            "Corpus sample: [('l', 'o', 'w', '_'), ('l', 'o', 'w', '_'), ('l', 'o', 'w', '_'), ('l', 'o', 'w', '_'), ('l', 'o', 'w', '_'), ('l', 'o', 'w', 'e', 's', 't', '_'), ('l', 'o', 'w', 'e', 's', 't', '_'), ('n', 'e', 'w', 'er', '_')]\n",
            "Updated vocab: {'l', 'w', 'i', 'o', 'n', 's', 'e', 't', 'r', '_', 'd', 'er'}\n",
            "\n",
            "Step 2: merge ('er', '_') → er_\n",
            "Corpus sample: [('l', 'o', 'w', '_'), ('l', 'o', 'w', '_'), ('l', 'o', 'w', '_'), ('l', 'o', 'w', '_'), ('l', 'o', 'w', '_'), ('l', 'o', 'w', 'e', 's', 't', '_'), ('l', 'o', 'w', 'e', 's', 't', '_'), ('n', 'e', 'w', 'er_')]\n",
            "Updated vocab: {'l', 'w', 'i', 'o', 'n', 's', 'e', 't', 'er_', 'r', '_', 'd', 'er'}\n",
            "\n",
            "Step 3: merge ('n', 'e') → ne\n",
            "Corpus sample: [('l', 'o', 'w', '_'), ('l', 'o', 'w', '_'), ('l', 'o', 'w', '_'), ('l', 'o', 'w', '_'), ('l', 'o', 'w', '_'), ('l', 'o', 'w', 'e', 's', 't', '_'), ('l', 'o', 'w', 'e', 's', 't', '_'), ('ne', 'w', 'er_')]\n",
            "Updated vocab: {'l', 'w', 'i', 'o', 'n', 's', 'ne', 'e', 't', 'er_', 'r', '_', 'd', 'er'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "toy_corpus = [\n",
        "    \"low\", \"low\", \"low\", \"low\", \"low\",\n",
        "    \"lowest\", \"lowest\",\n",
        "    \"newer\", \"newer\", \"newer\", \"newer\", \"newer\", \"newer\",\n",
        "    \"wider\", \"wider\", \"wider\",\n",
        "    \"new\", \"new\"\n",
        "]\n",
        "\n",
        "def get_stats(corpus):\n",
        "    pairs = defaultdict(int)\n",
        "    for word in corpus:\n",
        "        for i in range(len(word)-1):\n",
        "            pairs[(word[i], word[i+1])] += 1\n",
        "    return pairs\n",
        "\n",
        "def merge_pair(pair, corpus):\n",
        "    new_corpus = []\n",
        "    replacement = \"\".join(pair)\n",
        "    for word in corpus:\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            if i < len(word)-1 and (word[i], word[i+1]) == pair:\n",
        "                new_word.append(replacement)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        new_corpus.append(new_word)\n",
        "    return new_corpus\n",
        "\n",
        "class BPE:\n",
        "    def __init__(self, num_merges):\n",
        "        self.num_merges = num_merges\n",
        "        self.merges = []\n",
        "\n",
        "    def fit(self, corpus_words):\n",
        "        corpus = [list(word) + [\"_\"] for word in corpus_words]\n",
        "        for i in range(self.num_merges):\n",
        "            pairs = get_stats(corpus)\n",
        "            if not pairs:\n",
        "                break\n",
        "            best = max(pairs, key=pairs.get)\n",
        "            corpus = merge_pair(best, corpus)\n",
        "            self.merges.append(best)\n",
        "            print(f\"Step {i+1}: merge {best} → {''.join(best)} (vocab size={len(set(x for w in corpus for x in w))})\")\n",
        "        self.corpus = corpus\n",
        "\n",
        "    def segment(self, word):\n",
        "        tokens = list(word) + [\"_\"]\n",
        "        for pair in self.merges:\n",
        "            i = 0\n",
        "            while i < len(tokens)-1:\n",
        "                if (tokens[i], tokens[i+1]) == pair:\n",
        "                    tokens[i:i+2] = [\"\".join(pair)]\n",
        "                else:\n",
        "                    i += 1\n",
        "        return tokens\n",
        "\n",
        "print(\"=== Q3.2 Mini-BPE Learner ===\")\n",
        "toy_bpe = BPE(num_merges=10)\n",
        "toy_bpe.fit(toy_corpus)\n",
        "\n",
        "test_words = [\"new\", \"newer\", \"lowest\", \"widest\", \"newestest\"]\n",
        "for w in test_words:\n",
        "    print(f\"{w} → {toy_bpe.segment(w)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIEyEIcrlbkZ",
        "outputId": "315dec87-e57d-4eef-ce76-e6e8109e17fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Q3.2 Mini-BPE Learner ===\n",
            "Step 1: merge ('e', 'r') → er (vocab size=11)\n",
            "Step 2: merge ('er', '_') → er_ (vocab size=11)\n",
            "Step 3: merge ('n', 'e') → ne (vocab size=11)\n",
            "Step 4: merge ('ne', 'w') → new (vocab size=11)\n",
            "Step 5: merge ('l', 'o') → lo (vocab size=10)\n",
            "Step 6: merge ('lo', 'w') → low (vocab size=10)\n",
            "Step 7: merge ('new', 'er_') → newer_ (vocab size=11)\n",
            "Step 8: merge ('low', '_') → low_ (vocab size=12)\n",
            "Step 9: merge ('w', 'i') → wi (vocab size=11)\n",
            "Step 10: merge ('wi', 'd') → wid (vocab size=10)\n",
            "new → ['new', '_']\n",
            "newer → ['newer_']\n",
            "lowest → ['low', 'e', 's', 't', '_']\n",
            "widest → ['wid', 'e', 's', 't', '_']\n",
            "newestest → ['new', 'e', 's', 't', 'e', 's', 't', '_']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "paragraph = \"\"\"Data engineering collects and prepares data for analysis.\n",
        "It involves cleaning, transforming, and moving data between systems.\n",
        "Good data pipelines make analytics and machine learning reliable and repeatable.\n",
        "Engineers design systems for scale, latency, and fault tolerance.\"\"\"\n",
        "\n",
        "words = re.findall(r\"\\b\\w+\\b\", paragraph.lower())\n",
        "\n",
        "def get_stats(corpus):\n",
        "    pairs = defaultdict(int)\n",
        "    for word in corpus:\n",
        "        for i in range(len(word)-1):\n",
        "            pairs[(word[i], word[i+1])] += 1\n",
        "    return pairs\n",
        "\n",
        "def merge_pair(pair, corpus):\n",
        "    new_corpus = []\n",
        "    replacement = \"\".join(pair)\n",
        "    for word in corpus:\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            if i < len(word)-1 and (word[i], word[i+1]) == pair:\n",
        "                new_word.append(replacement)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        new_corpus.append(new_word)\n",
        "    return new_corpus\n",
        "\n",
        "class BPE:\n",
        "    def __init__(self, num_merges):\n",
        "        self.num_merges = num_merges\n",
        "        self.merges = []\n",
        "        self.vocab = set()\n",
        "\n",
        "    def fit(self, corpus_words):\n",
        "        corpus = [list(word) + [\"_\"] for word in corpus_words]\n",
        "        vocab = set(sym for w in corpus for sym in w)\n",
        "\n",
        "        for i in range(self.num_merges):\n",
        "            pairs = get_stats(corpus)\n",
        "            if not pairs:\n",
        "                break\n",
        "            best = max(pairs, key=pairs.get)\n",
        "            self.merges.append(best)\n",
        "            corpus = merge_pair(best, corpus)\n",
        "            vocab.add(\"\".join(best))\n",
        "            print(f\"Step {i+1}: merge {best} → {''.join(best)}\")\n",
        "        self.vocab = vocab\n",
        "        self.corpus = corpus\n",
        "\n",
        "    def segment(self, word):\n",
        "        tokens = list(word) + [\"_\"]\n",
        "        for pair in self.merges:\n",
        "            i = 0\n",
        "            while i < len(tokens)-1:\n",
        "                if (tokens[i], tokens[i+1]) == pair:\n",
        "                    tokens[i:i+2] = [\"\".join(pair)]\n",
        "                else:\n",
        "                    i += 1\n",
        "        return tokens\n",
        "\n",
        "para_bpe = BPE(num_merges=30)\n",
        "para_bpe.fit(words)\n",
        "\n",
        "print(\"\\nTop 5 merges:\", [\"\".join(p) for p in para_bpe.merges[:5]])\n",
        "longest = sorted(para_bpe.vocab, key=lambda x: len(x), reverse=True)[:5]\n",
        "print(\"5 longest tokens:\", longest)\n",
        "\n",
        "for w in [\"data\", \"transforming\", \"fault\", \"pipelines\", \"reliable\"]:\n",
        "    print(f\"{w} → {para_bpe.segment(w)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpHewDbylmhJ",
        "outputId": "c4908cf6-7627-40b5-d8b8-3ef4946457c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: merge ('i', 'n') → in\n",
            "Step 2: merge ('a', 'n') → an\n",
            "Step 3: merge ('s', '_') → s_\n",
            "Step 4: merge ('l', 'e') → le\n",
            "Step 5: merge ('a', 't') → at\n",
            "Step 6: merge ('d', '_') → d_\n",
            "Step 7: merge ('at', 'a') → ata\n",
            "Step 8: merge ('in', 'g') → ing\n",
            "Step 9: merge ('ing', '_') → ing_\n",
            "Step 10: merge ('an', 'd_') → and_\n",
            "Step 11: merge ('d', 'ata') → data\n",
            "Step 12: merge ('data', '_') → data_\n",
            "Step 13: merge ('e', 'n') → en\n",
            "Step 14: merge ('in', 'e') → ine\n",
            "Step 15: merge ('r', 'e') → re\n",
            "Step 16: merge ('f', 'o') → fo\n",
            "Step 17: merge ('fo', 'r') → for\n",
            "Step 18: merge ('y', 's') → ys\n",
            "Step 19: merge ('le', '_') → le_\n",
            "Step 20: merge ('en', 'g') → eng\n",
            "Step 21: merge ('eng', 'ine') → engine\n",
            "Step 22: merge ('engine', 'e') → enginee\n",
            "Step 23: merge ('enginee', 'r') → engineer\n",
            "Step 24: merge ('o', 'l') → ol\n",
            "Step 25: merge ('re', 'p') → rep\n",
            "Step 26: merge ('for', '_') → for_\n",
            "Step 27: merge ('an', 'a') → ana\n",
            "Step 28: merge ('ana', 'l') → anal\n",
            "Step 29: merge ('t', '_') → t_\n",
            "Step 30: merge ('r', 'an') → ran\n",
            "\n",
            "Top 5 merges: ['in', 'an', 's_', 'le', 'at']\n",
            "5 longest tokens: ['engineer', 'enginee', 'engine', 'data_', 'anal']\n",
            "data → ['data_']\n",
            "transforming → ['t', 'ran', 's', 'for', 'm', 'ing_']\n",
            "fault → ['f', 'a', 'u', 'l', 't_']\n",
            "pipelines → ['p', 'i', 'p', 'e', 'l', 'ine', 's_']\n",
            "reliable → ['re', 'l', 'i', 'a', 'b', 'le_']\n"
