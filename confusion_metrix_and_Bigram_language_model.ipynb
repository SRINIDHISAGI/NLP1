{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SRINIDHISAGI/NLP1/blob/main/confusion_metrix_and_Bigram_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Confusion matrix: rows=predicted, columns=actual\n",
        "conf_matrix = np.array([\n",
        "    [5, 10, 5],   # Cat predicted\n",
        "    [15, 20, 10], # Dog predicted\n",
        "    [0, 15, 10]   # Rabbit predicted\n",
        "])\n",
        "\n",
        "classes = [\"Cat\", \"Dog\", \"Rabbit\"]\n",
        "TP = np.diag(conf_matrix)\n",
        "FP = conf_matrix.sum(axis=1) - TP\n",
        "FN = conf_matrix.sum(axis=0) - TP\n",
        "\n",
        "# Per-class precision and recall\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "\n",
        "# Macro-averaged\n",
        "macro_precision = precision.mean()\n",
        "macro_recall = recall.mean()\n",
        "\n",
        "# Micro-averaged\n",
        "TP_total = TP.sum()\n",
        "FP_total = FP.sum()\n",
        "FN_total = FN.sum()\n",
        "micro_precision = TP_total / (TP_total + FP_total)\n",
        "micro_recall = TP_total / (TP_total + FN_total)\n",
        "\n",
        "# Print results\n",
        "print(\"Per-class Metrics:\")\n",
        "for i, cls in enumerate(classes):\n",
        "    print(f\"{cls}: Precision = {precision[i]:.3f}, Recall = {recall[i]:.3f}\")\n",
        "\n",
        "print(\"\\nMacro-averaged Metrics:\")\n",
        "print(f\"Precision = {macro_precision:.3f}, Recall = {macro_recall:.3f}\")\n",
        "\n",
        "print(\"\\nMicro-averaged Metrics:\")\n",
        "print(f\"Precision = {micro_precision:.3f}, Recall = {micro_recall:.3f}\")"
      ],
      "metadata": {
        "id": "naDyRuuLYSHS",
        "outputId": "42baa0ac-c5e5-4dc7-911f-56059a188098",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-class Metrics:\n",
            "Cat: Precision = 0.250, Recall = 0.250\n",
            "Dog: Precision = 0.444, Recall = 0.444\n",
            "Rabbit: Precision = 0.400, Recall = 0.400\n",
            "\n",
            "Macro-averaged Metrics:\n",
            "Precision = 0.365, Recall = 0.365\n",
            "\n",
            "Micro-averaged Metrics:\n",
            "Precision = 0.389, Recall = 0.389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Training corpus\n",
        "corpus = [\n",
        "    \" I love NLP \",\n",
        "    \" I love deep learning \",\n",
        "    \" deep learning is fun \"\n",
        "]\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1. Tokenize corpus\n",
        "# ----------------------------------------------------------\n",
        "tokenized = [sentence.split() for sentence in corpus]\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2. Count unigrams and bigrams\n",
        "# ----------------------------------------------------------\n",
        "unigram_counts = defaultdict(int)\n",
        "bigram_counts = defaultdict(int)\n",
        "\n",
        "for sent in tokenized:\n",
        "    for i, word in enumerate(sent):\n",
        "        unigram_counts[word] += 1\n",
        "        if i < len(sent) - 1:\n",
        "            bigram = (sent[i], sent[i+1])\n",
        "            bigram_counts[bigram] += 1\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3. Compute bigram probabilities (MLE)\n",
        "# P(w_i | w_{i-1}) = count(w_{i-1}, w_i) / count(w_{i-1})\n",
        "# ----------------------------------------------------------\n",
        "bigram_probs = {}\n",
        "for (w1, w2), count in bigram_counts.items():\n",
        "    bigram_probs[(w1, w2)] = count / unigram_counts[w1]\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4. Function to compute sentence probability\n",
        "# ----------------------------------------------------------\n",
        "def sentence_probability(sentence, bigram_probs, unigram_counts):\n",
        "    words = sentence.split()\n",
        "    prob = 1.0\n",
        "    for i in range(len(words) - 1):\n",
        "        w1, w2 = words[i], words[i+1]\n",
        "        if (w1, w2) in bigram_probs:\n",
        "            prob *= bigram_probs[(w1, w2)]\n",
        "        else:\n",
        "            prob *= 0  # unseen bigram â†’ probability 0 under MLE\n",
        "    return prob\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 5. Test sentences\n",
        "# ----------------------------------------------------------\n",
        "s1 = \" I love NLP \"\n",
        "s2 = \" I love deep learning \"\n",
        "\n",
        "p1 = sentence_probability(s1, bigram_probs, unigram_counts)\n",
        "p2 = sentence_probability(s2, bigram_probs, unigram_counts)\n",
        "\n",
        "print(\"P(S1 =\", s1, \") =\", p1)\n",
        "print(\"P(S2 =\", s2, \") =\", p2)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 6. Model preference\n",
        "# ----------------------------------------------------------\n",
        "if p1 > p2:\n",
        "    print(\"Model prefers Sentence 1:\", s1, \"because it has higher probability.\")\n",
        "elif p2 > p1:\n",
        "    print(\"Model prefers Sentence 2:\", s2, \"because it has higher probability.\")\n",
        "else:\n",
        "    print(\"Model considers both sentences equally probable.\")\n"
      ],
      "metadata": {
        "id": "Q-hofM97YnJE",
        "outputId": "50c9057e-f881-44a6-851e-552ad67db0c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(S1 =  I love NLP  ) = 0.5\n",
            "P(S2 =  I love deep learning  ) = 0.5\n",
            "Model considers both sentences equally probable.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "confusion_metrix_and-Bigram_language_model",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}